{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwZgqxFCdn+fh9EpCPEFTd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnc-br/ddf_common/blob/vertex_ai_test/vertex_ai_poc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LqDd-awJnx9",
        "outputId": "ac7f8e64-dac4-4d66-d72f-83630525de33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "executing checkout_branch ...\n",
            "b''\n",
            "main branch checked out as readonly. You may now use ddf_common imports\n"
          ]
        }
      ],
      "source": [
        "# This stub (ddfimport) allows the Ddf EE API to be imported.\n",
        "import sys\n",
        "!if [ ! -d \"/content/ddf_common_stub\" ] ; then git clone -b test https://github.com/tnc-br/ddf_common_stub.git; fi\n",
        "sys.path.append(\"/content/ddf_common_stub/\")\n",
        "import ddfimport\n",
        "\n",
        "# Use this line to import from a branch of the github repository.\n",
        "# It will git clone the git repository under a google drive path.\n",
        "# This allows you to modify the source files by opening the file view and\n",
        "# changing files under /content/gdrive/MyDrive/<branch_name>\n",
        "# ddfimport.ddf_source_control_pane()\n",
        "\n",
        "# Alternatively, you can use this line to import from Main.\n",
        "# If you import from Main, you will not be able to change files, but will not\n",
        "# need a Google Login for Google Drive.\n",
        "ddfimport.ddf_import_common()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import eeddf\n",
        "\n",
        "# This may prompt for your Google account credentials to authenticate for\n",
        "# Earth Engine. You may pass in an argument test_environment = True to access\n",
        "# test data.\n",
        "eeddf.initialize_ddf(test_environment = True)"
      ],
      "metadata": {
        "id": "ESPRca9_JtXR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade google-cloud-bigquery"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WODh7fQ4liQv",
        "outputId": "200f3e93-5034-4bde-b0b5-4bf0f729bc9b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.10/dist-packages (3.17.2)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.7.0)\n",
            "Requirement already satisfied: packaging>=20.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (23.2)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.8.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.31.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (1.62.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (3.20.3)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (2.27.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (1.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2024.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "_CLIENT = None\n",
        "_TEST_CONFIG = {\n",
        "    \"TABLE\" : \"eval_results\",\n",
        "    \"DATASET\" : \"harness_test_db\",\n",
        "    \"PROJECT\" : \"river-sky-386919\",\n",
        "}\n",
        "\n",
        "_PROD_CONFIG = {\n",
        "    \"TABLE\" : \"eval_results\",\n",
        "    \"DATASET\" : \"harness_prod_db\",\n",
        "    \"PROJECT\" : \"timberidprd\",\n",
        "}\n",
        "\n",
        "def get_config():\n",
        "  if eeddf.is_test_environment():\n",
        "    global _TEST_CONFIG\n",
        "    return _TEST_CONFIG\n",
        "  global _PROD_CONFIG\n",
        "  return _PROD_CONFIG\n",
        "\n",
        "def get_big_query_client():\n",
        "  global _CLIENT\n",
        "  if not _CLIENT:\n",
        "    _CLIENT = bigquery.Client(get_config()['PROJECT'])\n",
        "  return _CLIENT\n",
        "\n",
        "def read_eval_result(eval_id):\n",
        "  client = get_big_query_client()\n",
        "\n",
        "  # Set up SQL query\n",
        "  table_name = f\"{get_config()['DATASET']}.{get_config()['TABLE']}\"\n",
        "  query = f\"SELECT * FROM {table_name} WHERE eval_id = '{eval_id}'\"\n",
        "\n",
        "  # Execute the query\n",
        "  results = client.query_and_wait(query)\n",
        "  return results\n",
        "\n",
        "def insert_eval_result(eval, allow_overwrite=False):\n",
        "  client = get_big_query_client()\n",
        "\n",
        "  exists = read_eval_result(eval['eval_id']).total_rows\n",
        "  if exists and not allow_overwrite:\n",
        "    return [f\"eval_id {eval['eval_id']} already exists\"]\n",
        "\n",
        "  # Set up reference to table we write to.\n",
        "  table_ref = client.dataset(get_config()['DATASET']).table(get_config()['TABLE'])\n",
        "  table = client.get_table(table_ref)\n",
        "\n",
        "  # Automatically populate the timestamp field with the BigQuery commit time.\n",
        "  eval['completion_timestamp'] = 'AUTO'\n",
        "\n",
        "  # Insert data into BigQuery table.\n",
        "  errors = client.insert_rows(table, [eval])\n",
        "  return errors\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "lZWg2TROKOEJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def test_write():\n",
        "  eval_1 = {\n",
        "    \"eval_id\" : 'd',\n",
        "    \"baseline_id\" : \"baseline_1\",\n",
        "    \"experiment_id\" : \"fake_experiment_1\",\n",
        "    \"fraction_fraudulent\" : 0.5,\n",
        "    \"p_value_strategy\" : \"DEFAULT\",\n",
        "    \"eval_pipeline_version_id\" : \"v1\",\n",
        "    \"dataset_id\" : \"v1\",\n",
        "    \"p_value_threshold_100km\" : 0.03,\n",
        "    \"precision_100km\": 0.7,\n",
        "    \"recall_100km\" : 0.2,\n",
        "    \"auc_100km\": 0.6,\n",
        "    \"p_value_threshold_300km\" : 0.04,\n",
        "    \"precision_300km\": 0.8,\n",
        "    \"recall_300km\" : 0.4,\n",
        "    \"auc_300km\": 0.7,\n",
        "    \"p_value_threshold_500km\" : 0.03,\n",
        "    \"precision_500km\": 0.7,\n",
        "    \"recall_500km\" : 0.2,\n",
        "    \"auc_500km\": 0.6,\n",
        "  }\n",
        "\n",
        "  eval_2 = {\n",
        "    \"eval_id\" : 'e',\n",
        "    \"baseline_id\" : \"baseline_1\",\n",
        "    \"experiment_id\" : \"fake_experiment_1\",\n",
        "    \"fraction_fraudulent\" : 0.6,\n",
        "    \"p_value_strategy\" : \"DEFAULT\",\n",
        "    \"eval_pipeline_version_id\" : \"v1\",\n",
        "    \"dataset_id\" : \"v1\",\n",
        "    \"p_value_threshold_100km\" : 0.03,\n",
        "    \"precision_100km\": 0.8,\n",
        "    \"recall_100km\" : 0.3,\n",
        "    \"auc_100km\": 0.7,\n",
        "    \"p_value_threshold_300km\" : 0.05,\n",
        "    \"precision_300km\": 0.9,\n",
        "    \"recall_300km\" : 0.5,\n",
        "    \"auc_300km\": 0.8,\n",
        "    \"p_value_threshold_500km\" : 0.05,\n",
        "    \"precision_500km\": 0.8,\n",
        "    \"recall_500km\" : 0.8,\n",
        "    \"auc_500km\": 0.9,\n",
        "  }\n",
        "\n",
        "  eval_3 = {\n",
        "    \"eval_id\" : 'f',\n",
        "    \"baseline_id\" : \"baseline_1\",\n",
        "    \"experiment_id\" : \"fake_experiment_1\",\n",
        "    \"fraction_fraudulent\" : 0.5,\n",
        "    \"p_value_strategy\" : \"DEFAULT\",\n",
        "    \"eval_pipeline_version_id\" : \"v1\",\n",
        "    \"dataset_id\" : \"v1\",\n",
        "    \"p_value_threshold_100km\" : 0.3,\n",
        "    \"precision_100km\": 0.9,\n",
        "    \"recall_100km\" : 0.4,\n",
        "    \"auc_100km\": 0.5,\n",
        "    \"p_value_threshold_300km\" : 0.09,\n",
        "    \"precision_300km\": 0.4,\n",
        "    \"recall_300km\" : 0.3,\n",
        "    \"auc_300km\": 0.2,\n",
        "    \"p_value_threshold_500km\" : 0.04,\n",
        "    \"precision_500km\": 0.3,\n",
        "    \"recall_500km\" : 0.22,\n",
        "    \"auc_500km\": 0.612,\n",
        "  }\n",
        "\n",
        "  errors = insert_eval_result(eval_1)\n",
        "  assert len(errors) == 0\n",
        "\n",
        "  errors = insert_eval_result(eval_2)\n",
        "  assert len(errors) == 0\n",
        "\n",
        "  errors = insert_eval_result(eval_3)\n",
        "  assert len(errors) == 0\n",
        "\n",
        "  result = read_eval_result('a')\n",
        "  assert result.total_rows == 1\n",
        "\n",
        "  rows = read_eval_result('b')\n",
        "  assert result.total_rows == 1\n",
        "\n",
        "  rows = read_eval_result('c')\n",
        "  assert result.total_rows == 1\n",
        ""
      ],
      "metadata": {
        "id": "RrZ2fw6tKCKY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_write()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7of_mZ0KzPE",
        "outputId": "86668307-15ed-409e-fc2b-21a832d00ffe"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    }
  ]
}